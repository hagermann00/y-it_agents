# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ðŸ“š NOTEBOOKLM RESEARCH ACQUISITION â€” SOP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PURPOSE:  Populate NotebookLM with up to 500 sources per topic
# TOOL:     NotebookLM (native source upload)
# OUTPUT:   Research-ready notebook for Research Processor
# STATUS:   DRAFT â€” Structure for review
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

---

## NOTEBOOKLM SOURCE LIMITS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MAXIMUM SOURCES PER NOTEBOOK: 500                                      â”‚
â”‚                                                                          â”‚
â”‚  1 Source can be:                                                        â”‚
â”‚  â€¢ 1 PDF (any length)                                                    â”‚
â”‚  â€¢ 1 URL/webpage                                                         â”‚
â”‚  â€¢ 1 Google Doc                                                          â”‚
â”‚  â€¢ 1 Deep Research session (counts as 1 source!)                        â”‚
â”‚  â€¢ 1 Text paste                                                          â”‚
â”‚  â€¢ 1 YouTube video                                                       â”‚
â”‚                                                                          â”‚
â”‚  âš ï¸ Deep Research = comprehensive AI research on a query                 â”‚
â”‚     Generates substantial content but uses only 1 source slot            â”‚
â”‚     â†’ LEVERAGE THIS for efficient source usage                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## SOURCE ALLOCATION STRATEGY

### Recommended Distribution (500 sources max)

| Category | Sources | Purpose |
|----------|---------|---------|
| **Deep Research Sessions** | 20-40 | Heavy lifting â€” each returns substantial data |
| **Official Platform Docs** | 30-50 | Pricing, policies, terms of service |
| **Course/Guru Sales Pages** | 20-30 | CLAIM collection |
| **Reddit Threads** | 50-80 | OUTCOME stories, failures, real talk |
| **YouTube Videos** | 30-50 | Tutorials, reviews, "I quit" videos |
| **News/Articles** | 30-50 | Industry data, trend reports |
| **Forum/Community Posts** | 40-60 | Raw testimonials, Q&A threads |
| **Government/Official Sources** | 10-20 | RESOURCE collection (SBA, SCORE, etc.) |
| **Case Studies/Receipts** | 20-30 | Verified OUTCOMES with evidence |
| **Buffer/Overflow** | 50-100 | Topic-specific additions |

**Total: ~300-500 sources depending on topic complexity**

---

## DEEP RESEARCH STRATEGY

### What to Use Deep Research For

Deep Research is NotebookLM's AI-powered comprehensive research tool. Each session counts as **1 source** but returns **extensive compiled data**.

**Best queries for Deep Research:**

```
CLAIMS:
â†’ "Income claims and promises from [TOPIC] courses and gurus"
â†’ "Marketing tactics used to sell [TOPIC] training programs"

MECHANICS:
â†’ "Complete step-by-step process to start [TOPIC] business"
â†’ "Technical requirements and tools needed for [TOPIC]"

COSTS:
â†’ "Full cost breakdown for starting [TOPIC] business in 2024"
â†’ "Hidden fees and unexpected expenses in [TOPIC]"

OUTCOMES:
â†’ "Success stories in [TOPIC] with verified income proof"
â†’ "Failure stories and why people quit [TOPIC]"
â†’ "Average earnings and income distribution in [TOPIC]"

CONSTRAINTS:
â†’ "Platform policies and restrictions affecting [TOPIC]"
â†’ "Common reasons [TOPIC] businesses fail or get banned"

RESOURCES:
â†’ "Free alternatives to paid [TOPIC] courses"
â†’ "Government and nonprofit resources for small business"
```

### Deep Research Quota: 20-40 sessions

---

## SOURCE COLLECTION BY CLASSIFICATION

### CLAIMS (CH01) â€” Target: 40-60 sources

```
â–¡ Course/guru sales pages (15-20 URLs)
â–¡ YouTube income claim videos (10-15)
â–¡ Social media income screenshots (5-10 posts)
â–¡ Deep Research: "Income promises in [TOPIC]" (2-3 sessions)
â–¡ Email marketing sequences if available (5-10)
```

### MECHANICS (CH02) â€” Target: 30-50 sources

```
â–¡ Official platform documentation (10-15 URLs)
â–¡ Step-by-step tutorial videos (10-15)
â–¡ Deep Research: "How to start [TOPIC]" (2-3 sessions)
â–¡ Beginner guide articles (5-10)
â–¡ Platform help center pages (5-10)
```

### COSTS (CH03) â€” Target: 40-60 sources

```
â–¡ Pricing pages (screenshots/URLs) (15-20)
â–¡ "Real cost" breakdown videos (10-15)
â–¡ Reddit "how much I spent" threads (10-15)
â–¡ Deep Research: "True cost of [TOPIC]" (2-3 sessions)
â–¡ Fee documentation (payment processors, platforms) (5-10)
```

### OUTCOMES (CH04, CH05) â€” Target: 80-120 sources

```
â–¡ Reddit journey posts (r/[topic], sorted all time) (30-40)
â–¡ "I quit" YouTube videos (15-20)
â–¡ Success story interviews (5-10)
â–¡ Failure postmortems (15-20)
â–¡ Deep Research: "Success/failure rates in [TOPIC]" (3-5 sessions)
â–¡ Course refund testimonials (5-10)
â–¡ Income report posts/videos (10-15)
```

### CONSTRAINTS (CH05, CH06) â€” Target: 30-50 sources

```
â–¡ Platform terms of service (5-10)
â–¡ "Why I got banned" posts (10-15)
â–¡ Algorithm update articles (5-10)
â–¡ Deep Research: "Platform restrictions in [TOPIC]" (2-3 sessions)
â–¡ Payment hold policies (3-5)
â–¡ Legal/tax requirement docs (5-10)
```

### RESOURCES (CH07, CH08, Addendum) â€” Target: 30-50 sources

```
â–¡ Free course platforms (Coursera, etc.) (5-10)
â–¡ Government resources (SBA, SCORE) (5-10)
â–¡ Alternative business model comparisons (10-15)
â–¡ Deep Research: "Alternatives to [TOPIC]" (2-3 sessions)
â–¡ Career transition resources (5-10)
â–¡ Financial recovery guides (3-5)
```

---

## SOURCE QUALITY TIERS

### Tier 1: STRONG (Prioritize)
```
âœ… Official platform documentation
âœ… Government/nonprofit sources
âœ… Verified income proof with receipts
âœ… Published financial reports
âœ… Deep Research sessions (comprehensive)
```

### Tier 2: MEDIUM (Use with context)
```
â˜‘ï¸ Reddit posts with detail and coherence
â˜‘ï¸ YouTube videos from non-gurus
â˜‘ï¸ News articles from reputable outlets
â˜‘ï¸ Forum discussions with substantiation
```

### Tier 3: WEAK (Use sparingly, flag as weak)
```
âš ï¸ Anonymous claims without proof
âš ï¸ Social media flexes
âš ï¸ Guru testimonials (biased source)
âš ï¸ Older data (>2 years)
```

---

## SOURCE NAMING CONVENTION

When adding sources, use consistent naming:

```
[CLASSIFICATION]-[NUMBER]-[SHORT-DESCRIPTION]

Examples:
CLAIM-001-guru-income-promise
MECH-015-shopify-setup-tutorial
COST-032-facebook-ads-breakdown
OUTCOME-078-karen-failure-story
CONST-095-paypal-hold-policy
RSRC-112-score-free-mentoring
```

---

## RESEARCH PHASES

### Phase 1: Deep Research First (1-2 hours)
```
1. Run 20-30 Deep Research queries
2. Cover all 6 classifications
3. This gives you baseline coverage
4. ~20-30 source slots used, massive data collected
```

### Phase 2: Official Sources (1-2 hours)
```
1. Add all platform documentation
2. Add pricing pages (current, dated)
3. Add terms of service
4. Add payment processor policies
5. ~50-80 additional sources
```

### Phase 3: Community Sources (2-3 hours)
```
1. Reddit deep dives (sorted by controversial, top)
2. YouTube testimonials and reviews
3. Forum discussions
4. Facebook group screen captures
5. ~150-200 additional sources
```

### Phase 4: Gap Filling (1 hour)
```
1. Review coverage by classification
2. Identify weak areas
3. Targeted source collection
4. Verify all 6 classifications represented
5. ~50-100 additional sources
```

---

## COMPLETION CHECKLIST

Before marking research "complete":

```
Source Count:
â–¡ Minimum 200 sources added
â–¡ Maximum 500 sources (hard limit)
â–¡ Deep Research sessions: 20+

Classification Coverage:
â–¡ CLAIMS: 40+ sources
â–¡ MECHANICS: 30+ sources
â–¡ COSTS: 40+ sources
â–¡ OUTCOMES: 80+ sources
â–¡ CONSTRAINTS: 30+ sources
â–¡ RESOURCES: 30+ sources

Quality Balance:
â–¡ Tier 1 sources: 30%+ of total
â–¡ Loser outcomes outnumber winners 3:1
â–¡ Multiple archetypes represented
â–¡ Date range: not all from same year

Naming:
â–¡ All sources named per convention
â–¡ Classification prefix on all
```

---

## OUTPUT

When research collection is complete:

```
NotebookLM Notebook: "[TOPIC] Research"
â”œâ”€â”€ 200-500 sources uploaded
â”œâ”€â”€ Named with classification prefixes
â”œâ”€â”€ Deep Research sessions included
â””â”€â”€ Ready for Research Processor prompt
```

**Next Step:** Use NotebookLM Chat + `RESEARCH_PROCESSOR_PROMPT.md` to classify and route data.

---

*DRAFT v0.2 â€” NotebookLM SOP structure, pending finalization*
